2022.2.26
    1. 修复了在worker启动后为其添加新的event而出现segmentation fault的问题
        问题原因：worker的eventAdd与eventPoll并发时，由于他们都需要引用__events这个数据结构，当eventPoll因为eventAdd新添加的
        文件描述符而返回后，可能此时eventAdd还没有将Event指针加入__events，最终导致了eventPoll取到空指针，造成handleEvent部分
        访问异常。
        解决措施：将epoll_ctl新增、删除操作与__events更新操作封装为原子操作，使得epoll_wait能够取到正确的指针。对于删除操作而
        言，存在这种情况：epoll_wait返回后，删除操作再进行，此时epoll_wait可能无法正常获取到目标文件描述符(刚刚被删除)，因此需
        在eventPoll中处理文件描述符不存在的情况(__events中找不到时直接跳过)。
    2. 两个新问题：
        1) eventDel造成的segmentation fault：每次io worker响应后直接调用_event->getWorker()->eventDel，最终会出现segmentation
        fault，若将上述调用改为close(fd)，则不会出现段错误，因此判断应该是eventDel出现了问题。
        2) 有时候server初始化(此时worker未进入running状态，但可能已经初始化完毕了)时会直接产生段错误，使用gdb调试发现错误的原因是
        程序接收到了SIGSEGV信号(无效内存访问导致的)，导致该信号出现的源头为std::__atomic_base<bool>::operator=(bool)，暂未找到
        合适的解决办法。

2022.2.27
    1. 解决了26号的问题2，原因在于start失败时，释放资源部分没有判断worker是否已经创建完毕，因此造成了错误的引用。现在在stop函数与
    析构中均考虑了智能指针空引用(worker未被创建)的情况，不会再出现段错误。
    2. 解决了26号的问题1，原因在于addEvent时，先进行epollAddEvent，再设置Worker指针，此时若对event的handler发生在设置worker指针
    之前，在handler中引用worker指针就会发生段错误。
    3. 26号的两个段错误问题都在于worker中的event control事件和event wait事件都可以同时发生，若没有细致处理两类调用所引用的临界区
    资源，就会引发段错误，一个可行的解决办法是将event control事件延迟到下一次event wait去处理，由于event wait不会同时发生，这样就
    能保证每次event_wait与handle_event的在Event资源上的视角是一致的。

2022.2.28
    1. 对Event的销毁完全交给智能指针来做，现在外部事件只需要将Event从Epoll里删除即可，这样可以保证将要被handle的Event能够在正常
    处理结束后自行释放。

2022.3.1
    1. 添加了CMakeLists.txt，现在构建项目更方便。
    2. 添加了SocketStreamBuffer类，使得[i/o]stream可以用于读写socket。
    3. 实现了HttpRequest.cc部分类方法。
    4. 构思了Logger。

2022.3.2
    1. 测试了SocketStreamBuffer类，并简单包装了istream、ostream为SocketInputStream、SocketOutputStream，使得这两类流可以自动
    释放SocketStreamBuffer资源。
    2. 一个新问题：
        1) 在服务端关闭客户端链接后再使用SocketOutputStream写入字符，不会触发SIGPIPE，但是使用send发送则会触发SIGPIPE。
    3. 一次异常：
        1) IOEventHandler首次简单使用SocketOutputStream响应客户端时，发生过一次异常退出，但是没有提示段错误，后续用GDB测试再也
        没有发生异常退出的情况，推测该次异常退出来源于服务端没有处理SIGPIPE信号。

2022.3.3
    1. 三个问题：
        1) thread初始化放在try cache块中，会触发system_error，为了避免直接将worker的running属性设置为false，现在先将try catch
        块移除了。
        2) 原先accept中地址长度字段当初没有初始化，导致了accept直接返回-1，并且errno提示非法参数。现在直接将这两个参数改为NULL，
        异常情况似乎消失了。
        3) recv请求的时候，发现结束符都是\r\n，但是构造响应时，若加入\r\n则会导致响应不可解析，换成\r也是如此，只有一个\n才正常。
    2. 弄清了getline对分隔符的处理关系，完成了HttpRequest的解析部分。
    3. 初步设计了HttpResponse、Router与Rouserce。

2022.3.4
    1. C++类私有静态成员仍可以在类外用**作用域+私有成员名**的方式初始化，但是初始化时一定要根据该类构造函数的要求进行。
    2. 完成了HttpResponse与Router的大部分设计。
    3. 一个问题:
        1) HttpRequest对于部分错误的处理需要考虑让HttpResponse返回一个错误页，而不是让程序直接抛弃该请求，因此需重新修正。
    4. ResourceAccessor应当只处理本地资源，对于非本地资源，在HttpResponsor返回HttpResponse后，根据Body的资源类型进行转发。
    5. 需要详细考虑服务器对非本地资源的处理，具体地，当远程服务完成对请求的处理后，返回的响应是否可以直接回送给客户端，还是需要重
    新读取响应，设置部分字段后返回。如果是后者，那么目前HttpRequest的Body部分难以适配此功能。需要重新设计Resource，使其既能用于描
    述远程资源又能用于描述本地资源。(远程资源的body不需要由ResourceAccessor进行抽取了)

2022.3.5
    1. 将HttpResponse的body类型转变为istream的智能指针，同时ResourceAccessor根据给定资源提供对应的istream流。如此对于字符串、文件
    类型的body都有了统一接口(分别对应istringstream和ifstream)，其资源的释放(关闭文件等)交由智能指针自动处理。
    2. Router额外提供对错误页的访问支持，这是独立于依托uri访问的接口，Router可以直接根据错误码返回对应的错误页资源。返回的Resource中
    用ResourceType字段表示这个资源是否存在，若Type为INVALID，则表示资源不存在。
    3. 对于配置中没有定义错误页的情况，Responsor将直接根据默认错误页构造方法构造body。
    4. 完成了HttpResponse、Router(除Router::doLoad(std::string&)函数)、Resource的编码。
    5. 两个问题：
        1) 将某些setter/getter函数设置为内联函数后，会在链接时触发undefined reference错误。查阅资料：在一个文件中定义的内联函数不能
        在另一个文件中使用，它们通常放在头文件中共享。
        2) Router在构造函数中调用loadFrom时进行了二次加锁(第一次在loadFrom，第二次在doLoad中修改Mapper)，在运行时出现terminate called
        after throwing an instance of '__gnu_cxx::recursive_init_error'。但是换成递归锁之后情况并没有改善，暂时将loadFrom替换成无锁加
        载doLoad。
    6. 完成了Router的测试。

2022.3.6
    1. 完成了HttpRequest与HttpResponse的接口测试，将接口集成到IOEventHandler后，除了SIGPIPE导致的异常退出外，不会出现异常情况。server启
    动后用WebBench模拟5000个客户端持续不断的发送2分钟Http请求，利用top观察server的内存占用情况，未发现明显变化，在IOEventHandler::handle
    函数中插入new char语句后，再次用WebBench进行上述过程，内存占用逐渐变大，据此可能暂时不存在大内存泄漏。
    2. 修正了HttpUtils中大部分类的getter函数，为其加上了const。

2022.3.7
    1. 完成了Cache模块的部分实现。
    2. 两个问题：
        1) HttpResponse中没有考虑到MIME类型，需要在后续添加对MIME类型的判断(可以先直接复制Request中的MIME类型)。
        2) 当前IOEventHandler既处理客户端链接的IO，又处理请求解析与响应构造，缺乏一种将IO与计算分离的机制。

2022.3.8
    1. 完成了Cache模块的实现与测试。
    2. 取消了Router的单例模式，Router和ResourceAccessor交由Server统一初始化。由于现在IOEventHandler不仅仅处理IO，还需要根据HTTP请求构造
    HTTP响应，因此Server将会传递Router与ResourceAccessor给IOEVentHandler，使其能够正常根据请求构造响应。
    3. 重新组织了Server的函数，将初始化、清理、停止三类功能分离，并加入ResourceAccessor与Router的初始化，其中若启用了RedisCache功能，还会
    初始化RedisCache来构造CachedResourceAccessor。
    4. 将Cache模块加入后暂时正常运行，但是运行一段时间后响应变慢，目前原因不明，可能与锁有关？
    5. 将RedisCache关闭后，ResourceAccessor自动采用FileAccessor，但是随后运行出现段错误。

2022.3.9
    1. FileAccessor出现段错误的原因为未找到文件后没有及时设置错误码，只返回一个nullptr，此时调用者由于只根据错误码判断返回结果是否有效，因
    此没有感知到空指针。现在FileAccessor找不到目标文件或者打开失败时会设置错误码，并且调用者不仅判断错误码，还会再次判断返回的指针是否为空。
    2. 替换Accessor为FileAccessor后没有出现响应变慢的问题，因此可能响应变慢还是由CachedResourceAccessor导致的。
    3. 为Router添加真实存在的文件后，用WebBench模拟访问(5000 clients, 300s)检验FileAccessor对文件读取的功能是否正常，目前没有发现问题。
    4. 重新启用CachedResourceAccessor，但是禁用cache miss时拷贝文件内容到string的过程以排除拷贝过程带来的影响，继续用WebBench测试，没有出
    现昨天的响应变慢的情况。在cache miss部分多加一条put语句，用redic-cli确保执行，再次使用WebBench进行测试，也没有出现响应变慢的情况，问题
    应该出在复制文件内容到字符串。重新激活字符串赋值部分，但是仍走cache访问，这样在每个页面在第一次失效后就被缓存到Cache，可以判断get是否存
    在问题，再次用WebBench检测，过一段时间后出现响应变慢的情况，观察控制台输出发现仍然有从FileAccessor获取文件的输出，由此可能是get函数或文
    件拷贝出现的问题(由于速度变慢了，等待redis的响应存在超时返回的情况，因此从FileAccessor取文件不一定表示get有问题)。
    5. 去掉所有的cache操作，只留下文件内容拷贝部分，再次测试，没有出现异常。
    6. 响应变慢是因为获取不到timed_lock，超时后去访问FileLoader，根本原因在于RedisConnection中的get/put函数在获取到锁后没有处理好锁的释放
    ，具体地，当redisReply为0时(此时已经获取到锁)，没有unlock就直接返回false，导致RedisConnection不可用，致使后续线程访问锁时皆超时。修改
    后再用WebBench测试，响应变慢的现象消失，似乎没有问题了。
    7. HttpResponse中在正确获取到静态资源后没有加上状态码，现在当正确获取资源后会将状态码设置为200，在浏览器端能看到正确的状态码响应。

2022.3.10
    1. 完成了Log模块的设计与编码，测试时双缓冲部分出现段错误。
    2. 单独测试了Logger、LogStream、Buffer部分，目前没有发现问题。
    3. std::thread中的lambda表达式若要引用调用者的栈变量，一定要注意栈变量是否一直会存在，如果不是(比如detach了)，那么就要在lambda中用值传
    递的方式传递变量，若是引用传递则会造成段错误。
    4. 用100个线程模拟Log，出现死锁。

2022.3.11
    1. 3.10中死锁问题的关键在于可能所有producer(Logger调用者)notify结束后，consumer(AsyncLogger线程)才完成写操作，这样生产者和消费者同时进
    入等待的过程，因循环等待造成死锁。
    2. 参考了他人对双缓冲的设计，放宽__alternate的缓冲区为无穷大，并且每次AsyncLogger线程都将__alternate中的内容移动到本地变量后再对本地变量
    进行磁盘写入操作。这样只需要一个锁lockAll与条件变量，lockAll用于保护__current与__alternate，当__current满时直接将其内容移动(追加)到__al
    ternate中，并对AsyncLogger线程进行一次唤醒，否则也检查一下__alternate是否为空，若不为空则再次唤醒(防止出现log与写入速度差异过大导致唤醒丢
    失)。AsyncLogger接受到唤醒信号或超时时间后获取__lockAll，将__alternate中的内容与__current中的内容添加到local中，再释放__lockAll以便正常
    记录日志，随后进行日志持久化操作。
    3. 修正后用100个线程模拟日志记录，目前没发现问题。
    4. 将server中原有用cout输出的部分全部转换为log输出。

2022.3.14
    1. 原来的AsyncLogger似乎有内存泄露的问题？(因为log文件过大导致？)
    2. 现在稍微修改了currentAvailable4的判断条件，并将log输出给/dev/null，用8个线程模拟Worker打印日志，没有看到mem资源有明显变化。
    3. 将默认的log文件设置为/dev/null，防止在wsl下直接占用过多磁盘空间...
    4. 在AsyncLogger的析构函数中主动唤醒Logger线程，现在当server初始化出错时不需要等timeout再退出了。
    5. server将根据server构造前是否初始化logger来决定要不要设置log文件为默认log文件。
    6. 重新用WebBench测试，mem资源没有明显变化。猜测是磁盘空间不够导致了wsl直接用内存缓存log数据？
    7. main.cc中简单处理了SIGPIPE。
